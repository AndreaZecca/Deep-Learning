{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# RNN and Transformers\n"],"metadata":{"id":"p2mmcVtW1NUo"}},{"cell_type":"markdown","source":["In this lab lesson we will see how and when to use Recurrent Neural Networks and how to exploits pre-trained Transformers model like Bert.\n"],"metadata":{"id":"IVw8qZ4H1KsP"}},{"cell_type":"markdown","source":["## Task description\n","\n"],"metadata":{"id":"6f4UeT2u2vgq"}},{"cell_type":"markdown","source":["In this exercise we will try to classify subjectivity of text in sentences.\n","\n","We will use a collection of 103 Italian newspaper's articles labeled as Objective or Subjective. Each article is divided in sentences, which are consequently classified as either Subjective or Objective.\n","\n","You can find the data along with a more detailed description [here](https://github.com/francescoantici/SubjectivITA).\n","\n","We will be trying to create a model which is able to predict if a sentence contains subjectivity or it is fully objective.\n","\n"],"metadata":{"id":"cAUzFjP_3YAa"}},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"3hJ8KsB215Kp"}},{"cell_type":"code","source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report\n","from sklearn.utils.class_weight import compute_class_weight"],"metadata":{"id":"9QypPJoE1ylP","executionInfo":{"status":"ok","timestamp":1685888205647,"user_tz":-120,"elapsed":3446,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Data loading"],"metadata":{"id":"pTi3RT8Z3uON"}},{"cell_type":"markdown","source":["Please download the train, val and test files from the [repository](https://github.com/francescoantici/SubjectivITA/tree/main/datasets/sentences).\n","\n","After having uploaded the three files to the notebook, use this utility function to load the data."],"metadata":{"id":"qtmNGBY64aKX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"jigTojuHKCfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/Università/DL/Laboratories/L4/*.csv ./"],"metadata":{"id":"jgxsw9ZdxVYg","executionInfo":{"status":"ok","timestamp":1685888266562,"user_tz":-120,"elapsed":5250,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_data_split(split):\n","  \"\"\"\n","  Args:\n","    - split: the split of the data you want to load.\n","  Returns:\n","    - X, y data, where X is the array containing the sentences and y is the labels vector.\n","\n","  \"\"\"\n","  df = pd.read_csv(f\"sentences{split.capitalize()}.csv\")\n","  return df['FRASE'].values, df['TAG_FRASE'].values"],"metadata":{"id":"vvByVSDP4ln7","executionInfo":{"status":"ok","timestamp":1685888228625,"user_tz":-120,"elapsed":6,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sentences_train, labels_train = get_data_split(split = 'train')\n","sentences_val, labels_val = get_data_split(split = 'val')\n","sentences_test, labels_test = get_data_split(split = 'test')"],"metadata":{"id":"qdNzDM2W5PkG","executionInfo":{"status":"ok","timestamp":1685888269448,"user_tz":-120,"elapsed":582,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Data Inspection"],"metadata":{"id":"upAynpZE5grI"}},{"cell_type":"code","source":["sentences_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPFsk0cnNf-y","executionInfo":{"status":"ok","timestamp":1685888277265,"user_tz":-120,"elapsed":618,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"d9dc28bc-8a83-4033-bb92-f88be229526a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Prova estrema su TikTok:',\n","       'bambina di 10 anni in coma a Palermo, dichiarata la morte cerebrale.',\n","       'Inchiesta per istigazione al suicidio.', ...,\n","       'è quanto ha detto Guido Bertolaso, nuovo consulente della Lombardia per la campagna vaccinale regionale, nel corso di una conferenza stampa con il presidente Attilio Fontana e la vicepresidente Letizia Moratti.',\n","       'Non voglio soldi, faccio il volontario e mi sono abbassato lo stipendio: da un euro zero,',\n","       'ha aggiunto.'], dtype=object)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["labels_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MfCPdaFNpS5","executionInfo":{"status":"ok","timestamp":1685888279348,"user_tz":-120,"elapsed":9,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"ca1be5c6-4090-440b-ef83-c7388647b518"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['OGG', 'OGG', 'OGG', ..., 'OGG', 'SOG', 'OGG'], dtype=object)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## RNN"],"metadata":{"id":"a7-hGV0u5qux"}},{"cell_type":"markdown","source":["Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n","modeling sequence data such as time series or natural language.\n","\n","Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n","sequence, while maintaining an internal state that encodes information about the\n","timesteps it has seen so far.\n","\n","The Keras RNN API is designed with a focus on:\n","\n","- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n","`keras.layers.GRU` layers enable you to quickly build recurrent models without\n","having to make difficult configuration choices.\n","\n","- **Ease of customization**: You can also define your own RNN cell layer (the inner\n","part of the `for` loop) with custom behavior, and use it with the generic\n","`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n","prototype different research ideas in a flexible way with minimal code."],"metadata":{"id":"ywLGSfth7wrb"}},{"cell_type":"markdown","source":["There are three built-in RNN layers in Keras:\n","\n","1. `keras.layers.SimpleRNN`, a fully-connected RNN where the output from previous\n","timestep is to be fed to next timestep.\n","\n","2. `keras.layers.GRU`, first proposed in\n","[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n","\n","3. `keras.layers.LSTM`, first proposed in\n","[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)."],"metadata":{"id":"2cXxjtEM765h"}},{"cell_type":"markdown","source":["### Data pre-processsing\n","\n","We will use a tokenizer [function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) provided by Keras to map each token to an integer, so that the model is able to interpreter it."],"metadata":{"id":"B9Xg_LD_79uT"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","\n","lbl_to_idx_dict = {\"OGG\":0, \"SOG\":1}\n","\n","label_to_idx_f = np.vectorize(lbl_to_idx_dict.get)\n","\n","vocabulary_dim = 10000\n","\n","def get_tokenizer(x_train):\n","  tokenizer = Tokenizer(num_words = vocabulary_dim)\n","  tokenizer.fit_on_texts(x_train)\n","  return tokenizer\n","\n","tokenizer = get_tokenizer(sentences_train)"],"metadata":{"id":"dL-CjtIr782R","executionInfo":{"status":"ok","timestamp":1685888302352,"user_tz":-120,"elapsed":632,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KXuV0urhQ03c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import pad_sequences\n","\n","maxSentenceLen = 20\n","\n","generate_x = lambda x: pad_sequences(tokenizer.texts_to_sequences(x), maxlen = maxSentenceLen, padding = \"post\")\n","\n","x_train = generate_x(sentences_train)\n","x_test = generate_x(sentences_test)\n","x_val = generate_x(sentences_val)\n","\n","y_train = label_to_idx_f(labels_train)\n","y_test = label_to_idx_f(labels_test)\n","y_val = label_to_idx_f(labels_val)"],"metadata":{"id":"Zzt_qt9rbk-N","executionInfo":{"status":"ok","timestamp":1685888309661,"user_tz":-120,"elapsed":450,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["let's build a rnn baseline based on LSTM"],"metadata":{"id":"xokrUCPGqfvZ"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","\n","def get_rnn_model(input_shape, out_dim, vocabulary_dim):\n","  input = Input(shape=input_shape)\n","\n","  embedding_layer = Embedding(input_dim=vocabulary_dim, output_dim=64)(input)\n","\n","  lstm_1 = LSTM(128, return_sequences=True, recurrent_dropout = 0.2)(embedding_layer)\n","\n","  lstm_2 = LSTM(64, dropout = 0.2)(lstm_1)\n","\n","  output = Dense(out_dim)(lstm_2)\n","\n","  model = Model(input, output)\n","\n","  model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(1e-3), metrics = ['accuracy'])\n","  \n","  model.summary()\n","  \n","  return model"],"metadata":{"id":"GGB2xMC18iwW","executionInfo":{"status":"ok","timestamp":1685888321203,"user_tz":-120,"elapsed":484,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 5, restore_best_weights = True)"],"metadata":{"id":"LTbbDv8KzqKq","executionInfo":{"status":"ok","timestamp":1685888324471,"user_tz":-120,"elapsed":6,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model = get_rnn_model((20,), 2, vocabulary_dim)"],"metadata":{"id":"rwXMid6T_n5X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888330126,"user_tz":-120,"elapsed":3915,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"9fa17b78-934b-41b6-aebb-192036c66c51"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 20)]              0         \n","                                                                 \n"," embedding (Embedding)       (None, 20, 64)            640000    \n","                                                                 \n"," lstm (LSTM)                 (None, 20, 128)           98816     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense (Dense)               (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 788,354\n","Trainable params: 788,354\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(x_train, y_train, epochs=10, validation_data = (x_val, y_val), callbacks = [callback])"],"metadata":{"id":"qtnrwygRcmTt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888462918,"user_tz":-120,"elapsed":59811,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"8174f666-756f-4498-8483-c942121abb26"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","44/44 [==============================] - 21s 260ms/step - loss: 0.6150 - accuracy: 0.7041 - val_loss: 0.6641 - val_accuracy: 0.6233\n","Epoch 2/10\n","44/44 [==============================] - 9s 192ms/step - loss: 0.4526 - accuracy: 0.7884 - val_loss: 0.7615 - val_accuracy: 0.6930\n","Epoch 3/10\n","44/44 [==============================] - 8s 194ms/step - loss: 0.1271 - accuracy: 0.9578 - val_loss: 1.1384 - val_accuracy: 0.6605\n","Epoch 4/10\n","44/44 [==============================] - 6s 139ms/step - loss: 0.0422 - accuracy: 0.9871 - val_loss: 1.3477 - val_accuracy: 0.6744\n","Epoch 5/10\n","44/44 [==============================] - 6s 133ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 1.8792 - val_accuracy: 0.6930\n","Epoch 6/10\n","44/44 [==============================] - 4s 100ms/step - loss: 0.0206 - accuracy: 0.9964 - val_loss: 2.3025 - val_accuracy: 0.6512\n","Epoch 7/10\n","44/44 [==============================] - 4s 98ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 1.7351 - val_accuracy: 0.6698\n"]}]},{"cell_type":"markdown","source":["### Model evaluation\n","\n","For the model evaluation we will use the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function provided by scikit-learn, which will present a detailed report of the performances of the model evaluated on different metrics."],"metadata":{"id":"_SlpWhGHCiYc"}},{"cell_type":"code","source":["toLabels = np.vectorize(lambda e: \"OGG\" if e == 0 else \"SOG\")\n","\n","def evaluate_model(model, x_test, y_test):\n","  \"\"\"\n","  Args:\n","    - model: the model to use to make the prediction.\n","    - x_test: the sentences to label.\n","    - y_test: the actual labels.\n","  Returns:\n","    - The results of the evaluation of the model.\n","\n","  \"\"\"\n","  y_pred = np.argmax(model.predict(x_test), axis = -1)\n","  print(classification_report(toLabels(y_test), y_pred = toLabels(y_pred)))"],"metadata":{"id":"zYBXj27zCkkm","executionInfo":{"status":"ok","timestamp":1685888496916,"user_tz":-120,"elapsed":520,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["evaluate_model(model, x_test, y_test)"],"metadata":{"id":"m5UA2u5ADfgV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888550967,"user_tz":-120,"elapsed":51192,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"30032ac1-1b93-4321-8c59-3b412d1cb5d6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 2s 12ms/step\n","              precision    recall  f1-score   support\n","\n","         OGG       0.77      0.77      0.77       152\n","         SOG       0.54      0.55      0.54        75\n","\n","    accuracy                           0.70       227\n","   macro avg       0.66      0.66      0.66       227\n","weighted avg       0.70      0.70      0.70       227\n","\n"]}]},{"cell_type":"markdown","source":["#### Bidirectional RNNs\n","\n","For sequences other than time series (e.g. text), it is often the case that a RNN model\n","can perform better if it not only processes sequence from start to end, but also\n","backwards. For example, to predict the next word in a sentence, it is often useful to\n","have the context around the word, not only just the words that come before it.\n","\n","Keras provides an easy API for you to build such **bidirectional RNNs**: the\n","`keras.layers.Bidirectional` wrapper."],"metadata":{"id":"NbLopRmb-CN0"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Bidirectional, TimeDistributed\n","\n","def get_rnn_model_bd(input_shape, out_dim, vocabulary_length):\n","  input = Input(shape=input_shape)\n","\n","  embedding_layer = Embedding(input_dim=vocabulary_length, output_dim=64)(input)\n","\n","  lstm_1 = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout = 0.2))(embedding_layer)\n","\n","  lstm_2 = Bidirectional(LSTM(64, dropout = 0.2))(lstm_1)\n","\n","  output = Dense(out_dim)(lstm_2)\n","\n","  model = Model(input, output)\n","\n","  model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(1e-3), metrics = ['accuracy'])\n","  \n","  model.summary()\n","  \n","  return model"],"metadata":{"id":"_NxMWVoU9971","executionInfo":{"status":"ok","timestamp":1685888559989,"user_tz":-120,"elapsed":638,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model_bd = get_rnn_model_bd((20,), 2, vocabulary_dim)"],"metadata":{"id":"aB-tTuuKATXW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888570061,"user_tz":-120,"elapsed":1379,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"42c98289-fcc1-4199-c4e5-2af6ffeba91c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 20)]              0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 20, 64)            640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 20, 256)          197632    \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              164352    \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 258       \n","                                                                 \n","=================================================================\n","Total params: 1,002,242\n","Trainable params: 1,002,242\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model_bd.fit(x_train, y_train, epochs=10, validation_data = (x_val, y_val), callbacks = [callback])"],"metadata":{"id":"odtIo3d6jdC2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888675351,"user_tz":-120,"elapsed":99900,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"5f6b9836-e840-47de-f584-ca29c60ad318"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","44/44 [==============================] - 23s 339ms/step - loss: 0.6157 - accuracy: 0.7048 - val_loss: 0.6614 - val_accuracy: 0.6233\n","Epoch 2/10\n","44/44 [==============================] - 12s 274ms/step - loss: 0.4354 - accuracy: 0.7848 - val_loss: 0.7456 - val_accuracy: 0.7023\n","Epoch 3/10\n","44/44 [==============================] - 11s 263ms/step - loss: 0.1120 - accuracy: 0.9564 - val_loss: 1.0542 - val_accuracy: 0.6698\n","Epoch 4/10\n","44/44 [==============================] - 9s 214ms/step - loss: 0.0282 - accuracy: 0.9936 - val_loss: 1.2210 - val_accuracy: 0.7302\n","Epoch 5/10\n","44/44 [==============================] - 8s 183ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 1.1932 - val_accuracy: 0.7116\n","Epoch 6/10\n","44/44 [==============================] - 9s 213ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.2138 - val_accuracy: 0.7023\n","Epoch 7/10\n","44/44 [==============================] - 9s 203ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 1.4399 - val_accuracy: 0.7116\n","Epoch 8/10\n","44/44 [==============================] - 8s 179ms/step - loss: 9.9763e-04 - accuracy: 1.0000 - val_loss: 2.5711 - val_accuracy: 0.6977\n","Epoch 9/10\n","44/44 [==============================] - 9s 210ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 2.1920 - val_accuracy: 0.6558\n"]}]},{"cell_type":"code","source":["evaluate_model(model_bd, x_test, y_test)"],"metadata":{"id":"CEfvrHfRjgtt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685888725618,"user_tz":-120,"elapsed":43153,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"fa7bc7df-0e35-4683-8059-8d84500e7f81"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 1s 14ms/step\n","              precision    recall  f1-score   support\n","\n","         OGG       0.82      0.74      0.78       152\n","         SOG       0.56      0.68      0.61        75\n","\n","    accuracy                           0.72       227\n","   macro avg       0.69      0.71      0.70       227\n","weighted avg       0.74      0.72      0.72       227\n","\n"]}]},{"cell_type":"markdown","source":["## Transformers"],"metadata":{"id":"H9e0VvSD52rV"}},{"cell_type":"markdown","source":["Transformers are deep neural networks that over the last years achieved state of the art performances in several tasks.\n","\n","Transformers replaces CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self attention allows Transformers to easily transmit information across the input sequences.\n","\n","For the transformer model implementation we will rely on a Python library called `transformers`, which provides an API inteface to several pre-trained models for fine-tuning or transfer-learning purposes."],"metadata":{"id":"5U_P9h2t59Bw"}},{"cell_type":"code","source":["!pip3 install transformers \n","from transformers import AutoTokenizer, TFBertModel"],"metadata":{"id":"bVjERW866TJp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682526117808,"user_tz":-120,"elapsed":15097,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"5aef9684-8dc7-4511-a936-8e99c9161e56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"markdown","source":["### Pre-trained model\n","\n","For this task we will use a pre-trained language model called [AlBERTo](github.com/marcopoli/AlBERTo-it). AlBERTo is a BERT model trained for the Italian language. In particular, AlBERTo is focused on the language used in social networks, specifically on Twitter. Due to the language and the type of data present in the dataset AlBERTo is the best fit for this kind of task.\n","\n","You can find the pre-trained model in [huggingface](https://huggingface.co/bert-base-multilingual-cased?text=mi+piace+il+%5BMASK%5D), which is an open repository for pre-trained architectures, available in both pytorch and tensorflow (depending on the developers)."],"metadata":{"id":"YQfr8j8OtqEy"}},{"cell_type":"markdown","source":["### Data pre-processing"],"metadata":{"id":"kM0ZfujT6-J2"}},{"cell_type":"markdown","source":["Transoformes must recieve input in a standard format, namely divided in `input_ids`, `token_type_ids`, `attention_mask`."],"metadata":{"id":"r__5kvn8BPle"}},{"cell_type":"code","source":["def prepare_data_bert(x, y, maxSentenceLen = maxSentenceLen):\n","  \"\"\"\n","  Args:\n","    - x: the sentences to label.\n","    - y: the actual labels.\n","    - maxSentenceLen: The maximum length of the sentences, it is used as a truncation length\n","  Returns:\n","    - A tuple with the input to feed into a transformers model, namely ((input_ids, attention_mask, token_type_ids), categorical_labels).\n","\n","  \"\"\"\n","  pad = tf.keras.preprocessing.sequence.pad_sequences\n","  tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","  dataFields = {\n","          \"input_ids\": [],\n","          \"token_type_ids\": [],\n","          \"attention_mask\": [],\n","          \"subjectivity\": []\n","      }\n","  lbls = {\n","      'SOG' : 1.0,\n","      'OGG' : 0.0\n","  }\n","  for i in range(len(x)):\n","      data = tokenizer(x[i])\n","      padded = pad([data['input_ids'], data['attention_mask'], data['token_type_ids']], padding = 'post', maxlen = maxSentenceLen)\n","      dataFields['input_ids'].append(padded[0])\n","      dataFields['attention_mask'].append(padded[1])\n","      dataFields['token_type_ids'].append(padded[-1])\n","      dataFields['subjectivity'].append(lbls[y[i]])\n","  \n","  for key in dataFields:\n","      dataFields[key] = np.array(dataFields[key])\n","  \n","  return [dataFields[\"input_ids\"], dataFields[\"token_type_ids\"], dataFields[\"attention_mask\"]], dataFields[\"subjectivity\"]"],"metadata":{"id":"lHR30lup67iO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train_bert, y_train_bert = prepare_data_bert(sentences_train, labels_train)\n","x_val_bert, y_val_bert = prepare_data_bert(sentences_val, labels_val)\n","x_test_bert, y_test_bert = prepare_data_bert(sentences_test, labels_test)"],"metadata":{"id":"DBFNY98JuNv0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"B8oDhftmAgoj"}},{"cell_type":"markdown","source":["#### Try it yourself! \n","\n","Try to implement the model to fine tune AlBERTo model. \n"],"metadata":{"id":"vYdMNWSUu3fP"}},{"cell_type":"markdown","source":["The bert model takes as input three tensors of type tf.int32 and of shape (maxSentenceLen, ). \n","It returns as output two tensors, a simple model output -> modelOutput[0] and a pooled output -> modelOutput[-1].\n","You should work with the pooled output, so when you call the bert model as a layer in your network use it as follows:\n","\n","`bertModel = TFBertModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")(inputs)[-1]`"],"metadata":{"id":"W3kuOggyvs9r"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Concatenate\n","\n","def create_transformers_model(input_shape, out_dim):\n","  \"\"\"\n","  It should return the model instance to fine-tune the transformer model\n","  \"\"\"\n","  \n","  input_ids = Input(shape=input_shape, name=\"input_ids\", dtype=tf.int32)\n","  token_type_ids = Input(shape=input_shape, name=\"token_type_ids\", dtype=tf.int32)\n","  attention_mask = Input(shape=input_shape, name=\"attention_mask\", dtype=tf.int32)\n","  # inputs = Concatenate()([input_ids, token_type_ids, attention_mask])\n","\n","  bertModel = TFBertModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[-1]\n","  \n","  out = Dense(out_dim)(bertModel)\n","\n","  model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n","  model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(1e-3), metrics = ['accuracy'])\n","  print(model.summary())\n","  return model"],"metadata":{"id":"dcz-g_bfAf-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model = create_transformers_model((maxSentenceLen,), 2)"],"metadata":{"id":"kO7hJD-Muwj3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682528034260,"user_tz":-120,"elapsed":12002,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"4f0aac3b-e48f-4c55-ac7d-7f4e015910cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertModel.\n","\n","All the layers of TFBertModel were initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 20)]         0           []                               \n","                                                                                                  \n"," attention_mask (InputLayer)    [(None, 20)]         0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 20)]         0           []                               \n","                                                                                                  \n"," tf_bert_model_6 (TFBertModel)  TFBaseModelOutputWi  184345344   ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_mask[0][0]',         \n","                                tentions(last_hidde               'token_type_ids[0][0]']         \n","                                n_state=(None, 20,                                                \n","                                768),                                                             \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," dense_4 (Dense)                (None, 2)            1538        ['tf_bert_model_6[0][1]']        \n","                                                                                                  \n","==================================================================================================\n","Total params: 184,346,882\n","Trainable params: 184,346,882\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["Normally, pre-trained models shouldn't be fine tuned for long, otherwise you could alter the prior knowledge of the model and result in poor results, also, given the dimension of the model you can easily overfit on the new data. \n","For Bert fine tuning it is reccomended to use a low `Learning Rate (1e-5)` and to train for not more than 4 `epochs`."],"metadata":{"id":"pzPVrn-KxDA0"}},{"cell_type":"code","source":["history = bert_model.fit(x_train_bert, \n","                         y_train_bert, \n","                         epochs=4, \n","                         validation_data = (x_val_bert, y_val_bert), \n","                         batch_size = 16, \n","                         callbacks = [callback]\n","                         )"],"metadata":{"id":"ukHU9hO7k3Ut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682528178535,"user_tz":-120,"elapsed":125375,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"}},"outputId":"b19c6861-6d84-48e9-a647-c4b202104001"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","88/88 [==============================] - 77s 304ms/step - loss: 1.7218 - accuracy: 0.6490 - val_loss: 0.6625 - val_accuracy: 0.6233\n","Epoch 2/4\n","88/88 [==============================] - 14s 154ms/step - loss: 0.6493 - accuracy: 0.6812 - val_loss: 0.6955 - val_accuracy: 0.3767\n","Epoch 3/4\n","88/88 [==============================] - 13s 153ms/step - loss: 0.6383 - accuracy: 0.6848 - val_loss: 0.7239 - val_accuracy: 0.6233\n","Epoch 4/4\n","88/88 [==============================] - 11s 128ms/step - loss: 0.6430 - accuracy: 0.6919 - val_loss: 0.7885 - val_accuracy: 0.6233\n"]}]},{"cell_type":"markdown","source":["### Model evaluation"],"metadata":{"id":"prSfUspMDOFR"}},{"cell_type":"code","source":["evaluate_model(bert_model, x_test_bert, y_test_bert)"],"metadata":{"id":"-VZGxpniDmRc"},"execution_count":null,"outputs":[]}]}